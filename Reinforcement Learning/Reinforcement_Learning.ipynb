{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment, Agent, Reward, Action\n",
        "\n",
        "In reinforcement learning (RL), these components interact to enable learning through trial and error in dynamic environments.\n",
        "\n",
        "**Environment in RL**\n",
        "\n",
        "It represents the external system or context where the agent operates. It could be a physical space, a simulated world, or even an abstract problem. The environment responds to the actions of the agent and provides feedback or observations in response to those actions.\n",
        "\n",
        "**Agent in RL**\n",
        "\n",
        "The agent is the decision-making entity that interacts with the environment. It perceives the state of the environment, selects actions, and receives feedback in the form of rewards or penalties from the environment based on those actions.\n",
        "\n",
        "**Action in RL**\n",
        "\n",
        "Actions are decisions or moves made by the agent within the environment. These actions impact the state of the environment and may lead to new observations or states.\n",
        "\n",
        "**Reward in RL**\n",
        "\n",
        "Rewards are numerical values provided by the environment to the agent after it takes an action. These rewards serve as feedback to the agent, indicating how good or bad the action was with respect to the agent's goal. The agent's objective is typically to maximize the cumulative reward it receives over time.\n",
        "\n",
        "The goal of the agent in reinforcement learning is to learn a policy, a set of rules or strategies, that maximizes the total expected cumulative reward over time. The agent learns by exploring different actions and observing the consequences (rewards) in the environment, then updating its decision-making strategy based on these experiences.\n",
        "\n",
        "In essence, reinforcement learning involves the agent learning to make better decisions by interacting with an environment, receiving feedback (rewards), and adjusting its behavior to maximize long-term reward."
      ],
      "metadata": {
        "id": "x7_inP0dgrFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "4GI1v2_JfOdV",
        "outputId": "08b08620-a37a-41ae-977d-ce11c7f76efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/78.6 kB\u001b[0m \u001b[31m891.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=85397e3a905080481983c33aa08e6f2eca898bb72e1635eb33fb35588d27ad39\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built typing\n",
            "Installing collected packages: typing\n",
            "Successfully installed typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install typing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "class Environment:\n",
        "    # Initializing the environment\n",
        "    def __init__(self):\n",
        "        # Total steps to complete the game\n",
        "        self.steps_left = 20\n",
        "\n",
        "    # Observation of the environment\n",
        "    def get_observation(self) -> List[float]:\n",
        "        # For this example, a list of three float values for observation\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\n",
        "    # Possible actions the agent can take\n",
        "    def get_actions(self) -> List[int]:\n",
        "        # The agent can take two actions: 0 and 1\n",
        "        return [0, 1]\n",
        "\n",
        "    # Check if the game is over\n",
        "    def is_done(self) -> bool:\n",
        "        # The game ends when the steps are completed\n",
        "        return self.steps_left == 0\n",
        "\n",
        "    # Implementing an action and providing a reward\n",
        "    def action(self, action: int) -> float:\n",
        "        # If the game is already over, raise an exception\n",
        "        if self.is_done():\n",
        "            raise Exception(\"Game is over\")\n",
        "\n",
        "        # Decrement the number of steps left\n",
        "        self.steps_left -= 1\n",
        "\n",
        "        # Return a random reward for the action taken\n",
        "        return random.random()\n"
      ],
      "metadata": {
        "id": "gdV9l6d7hFJy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code :\n",
        "\n",
        "Observes the environment\n",
        "\n",
        "Takes decision based in the environment\n",
        "\n",
        "Sends the action to the environment\n",
        "\n",
        "Rewards the upcoming steps"
      ],
      "metadata": {
        "id": "7_wcJUhAhl62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        # Starting score point for the agent is 0.0\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def step(self, env: Environment):\n",
        "        # Getting the current observation from the environment\n",
        "        current_obs = env.get_observation()\n",
        "        print(\"Observation {}\".format(current_obs))\n",
        "\n",
        "        # Getting available actions from the environment\n",
        "        actions = env.get_actions()\n",
        "        print(actions)\n",
        "\n",
        "        # Taking a random action and receiving a reward from the environment\n",
        "        reward = env.action(random.choice(actions))\n",
        "\n",
        "        # Accumulating the reward obtained from the action\n",
        "        self.total_reward += reward\n",
        "        print(\"Total Reward {}\".format(self.total_reward))\n",
        "\n"
      ],
      "metadata": {
        "id": "9NrkIECwhiRC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = Environment()\n",
        "    agent = Agent()\n",
        "    i=0\n",
        "\n",
        "    while not env.is_done():\n",
        "        i=i+1\n",
        "        print(\"Steps {}\".format(i))\n",
        "        agent.step(env)\n",
        "\n",
        "    print(\"Total reward got: %.4f\" % agent.total_reward)\n",
        "\n",
        "\n",
        "print(\"Max value element : \", (agent.total_reward))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T32yrqnlh6Eu",
        "outputId": "3e0ef8af-af4a-454e-90d5-7aa173e670be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps 1\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 0.38418183995796384\n",
            "Steps 2\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.2233808814117078\n",
            "Steps 3\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 1.9314806451884\n",
            "Steps 4\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 2.5265748958486545\n",
            "Steps 5\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.4167193819439925\n",
            "Steps 6\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 3.866090981480984\n",
            "Steps 7\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.862868630313203\n",
            "Steps 8\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 4.8685887542693225\n",
            "Steps 9\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 5.857826599098511\n",
            "Steps 10\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.721536992530635\n",
            "Steps 11\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 6.9009392356067805\n",
            "Steps 12\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.323914682859593\n",
            "Steps 13\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 7.583330917203812\n",
            "Steps 14\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 8.486211872348365\n",
            "Steps 15\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 8.614745241632452\n",
            "Steps 16\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 9.261828848911271\n",
            "Steps 17\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 9.807965599107646\n",
            "Steps 18\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 9.88923281783795\n",
            "Steps 19\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 10.343111529012132\n",
            "Steps 20\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total Reward 10.463378148157817\n",
            "Total reward got: 10.4634\n",
            "Max value element :  10.463378148157817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tightening the code involves making it more precise and optimized. Here are a couple of ways to achieve that:\n",
        "\n",
        "**Strict Reward Points**\n",
        "\n",
        "Define a specific reward system where each correct step yields a defined number of points. For instance, you might assign a reward of +1 for every correct step taken by the agent towards the goal. This makes the reward system more defined and consistent.\n",
        "\n",
        "\n",
        "**Define Steps and Goals**\n",
        "\n",
        "Establish clear steps or a sequence of actions that the agent needs to take to reach a particular goal or complete the environment. This involves setting specific objectives or milestones that the agent should achieve within the environment, guiding it towards a defined target or goal state.\n",
        "By implementing these strategies, the code becomes more structured, defining the reward system and the steps required for the agent to accomplish its objectives within the environment. This enhances clarity, precision, and control over the agent's behavior and interactions within the environment."
      ],
      "metadata": {
        "id": "-lsnyS0YikKd"
      }
    }
  ]
}